version: '3'

services:

  elasticsearch:
    container_name: elasticsearch1
    build:
      context: elasticsearch/
    volumes:
      # docker-compose down也不会重新加载filebeat数据了,设置的密码也能保存
      - ./elasticsearch/data:/usr/share/elasticsearch/data
      - ./elasticsearch/logs:/usr/share/elasticsearch/logs
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
      - ./elasticsearch/config/elastic-certificates.p12:/usr/share/elasticsearch/config/elastic-certificates.p12:ro
    ports:
      - "9200:9200"
    stdin_open: true
    tty: true
    logging:
      #driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"

    environment:
      #- node.name=es01
      #- cluster.name=es-docker-cluster
      #- discovery.seed_hosts=es02,es03
      #- cluster.initial_master_nodes=es01,es02,es03
      #- bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - "ELASTIC_PASSWORD=changeme"
      #ES_JAVA_OPTS: "-Xmx256m -Xms256m"

      #XPACK.SECURITY.ENABLED: "true"
      #ELASTIC_PASSWORD: "elk"
      #有下面的配置有作用
      #ELASTIC_PASSWORD_FILE: "/run/secrets/auth_conf_hello.log"

    networks:
      - elk

  logstash:
    container_name: logstash1
    build:
      context: logstash/
    ulimits:
      memlock:
        soft: -1
        hard: -1
    volumes:
      - ./logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./logstash/pipeline:/usr/share/logstash/pipeline:ro
      - ./logstash/geoip/GeoLite2-City.mmdb:/usr/share/logstash/geoip/GeoLite2-City.mmdb
    ports:
      #- "5000:5000" # 只能有一个tcp
      - "5044:5044"
      - "9600:9600/udp"
      #- "12201:12201/udp"
    stdin_open: true
    tty: true
    logging:
      #driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"
    environment:
      LS_JAVA_OPTS: "-Xmx256m -Xms256m"
    networks:
      - elk
    links:
      - elasticsearch:elasticsearch
    depends_on:
      - elasticsearch

  kibana:
    container_name: kibana1
    build:
      context: kibana/
    volumes:
      - ./kibana/config/:/usr/share/kibana/config:ro
    ports:
      - "5601:5601"
    environment:
      ELASTICSEARCH_URL: http://elasticsearch:9200
      XPACK_SECURITY_ENABLED: "true"
      ELASTICSEARCH_USERNAME: "elastic"
      ELASTICSEARCH_PASSWORD: "changeme"
      # 解决连接到localhost:9200问题
      ELASTICSEARCH_HOSTS: '["http://elasticsearch:9200"]'
    stdin_open: true
    tty: true
    logging:
      #driver: "json-file"
      options:
        max-size: "10m"
        max-file: "50"
    networks:
      - elk
    links:
      - elasticsearch:elasticsearch
    depends_on:
      - elasticsearch

  filebeat:
    user: root
    container_name: filebeat1
    build:
      context: filebeat/
    links:
      - logstash:logstash
      - elasticsearch:elasticsearch
    depends_on:
      - logstash
      - elasticsearch
    #command: ./filebeat -e -strict.perms=false --path.config multi_enable/json
    # filebeat读过的文件就不会再读了,即使用了不同的配置文件.这种尝试失败
      # 指定文件名启动,但filebeat是以目录+必须filebeat.yml的启动方式,所以加此目录
      #- ./filebeat/config:/usr/share/filebeat/multi_enable
    volumes:
      - /var/run/docker.sock:/host_docker/docker.sock
      - /var/lib/docker:/host_docker/var/lib/docker
      # 测试logs
      #- ./logs:/usr/share/filebeat/mylogs
      # 此项目的web,nginx日志
      - ../Dockerfiles/nginx/log:/usr/share/filebeat/mylogs
      # 默认command时,要指定挂载一个filebeat.yml ok
      #- ./filebeat/config/modules/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - ./filebeat/config/json/filebeat.yml:/usr/share/filebeat/filebeat.yml
      - ./filebeat/modules.d:/usr/share/filebeat/modules.d
    command: ["--strict.perms=false"]
    ulimits:
      memlock:
        soft: -1
        hard: -1
    stdin_open: true
    tty: true
    networks:
      - elk
    #deploy:
      #mode: global
    logging:
      #driver: "json-file"
      #限制,一次性10m
      options:
        max-size: "10m"
        max-file: "50"
networks:

  elk:
    driver: bridge
